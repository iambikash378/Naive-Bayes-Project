{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import twitter_samples\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/adbik/nltk_data',\n",
       " '/usr/nltk_data',\n",
       " '/usr/share/nltk_data',\n",
       " '/usr/lib/nltk_data',\n",
       " '/usr/share/nltk_data',\n",
       " '/usr/local/share/nltk_data',\n",
       " '/usr/lib/nltk_data',\n",
       " '/usr/local/lib/nltk_data']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('twitter_samples')\n",
    "nltk.data.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of positive tweets are :  5000\n",
      "The length of positive tweets are :  5000\n"
     ]
    }
   ],
   "source": [
    "print(\"The length of positive tweets are : \", len(positive_tweets))\n",
    "print(\"The length of positive tweets are : \", len(negative_tweets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3124\n",
      "Hey Iphone dreamer : tanisha1nonly :) Wanna get iPh0ne 6 for FREE? Checkout my bi0. Thanks https://t.co/IxFXJi9YJ8\n",
      "SICK :(\n"
     ]
    }
   ],
   "source": [
    "random_tweet = np.random.randint(0, 5000)\n",
    "print(random_tweet)\n",
    "\n",
    "print(positive_tweets[random_tweet])\n",
    "print(negative_tweets[random_tweet])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing the Tweets\n",
    "\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    " # We first remove hyperlinks, twitter marks and styles using regular expressions\n",
    "\n",
    "def remove_unwanted(tweet):\n",
    "    # Removing hashtags\n",
    "    clean_tweet = re.sub(r'#','',tweet)\n",
    "\n",
    "    #Removing hyperlinks\n",
    "    clean_tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', clean_tweet)\n",
    "    \n",
    "    \n",
    "    return clean_tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3124\n",
      "Hey Iphone dreamer : tanisha1nonly :) Wanna get iPh0ne 6 for FREE? Checkout my bi0. Thanks https://t.co/IxFXJi9YJ8\n",
      "Hey Iphone dreamer : tanisha1nonly :) Wanna get iPh0ne 6 for FREE? Checkout my bi0. Thanks \n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(random_tweet)\n",
    "print(positive_tweets[random_tweet])\n",
    "removed_tweet = remove_unwanted(positive_tweets[random_tweet])\n",
    "print(removed_tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then tokenize the string i.e. split the string into individual words\n",
    "\n",
    "tokenizer = TweetTokenizer(preserve_case = False, strip_handles=True, reduce_len=True)\n",
    "\n",
    "def tokenize_tweet(tweet):\n",
    "    tokened_tweet = tokenizer.tokenize(tweet)\n",
    "    return tokened_tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now remove stop words and punctuations\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "#importing English stop words list from NLTK\n",
    "stopwords_eng = stopwords.words('english')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "def remove_stopwords_and_punctuations(tokened_tweets):\n",
    "    clean_tokens = []\n",
    "\n",
    "    for word in tokened_tweets:\n",
    "        if(word not in stopwords_eng and word not in punctuations):\n",
    "            clean_tokens.append(word)\n",
    "    \n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now do stemming, which means to convert a word into its general form i.e. simple present form\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def get_stem(tweets_clean):\n",
    "    tweets_stem = []\n",
    "    \n",
    "    for word in tweets_clean:\n",
    "        stem_word = stemmer.stem(word)\n",
    "        tweets_stem.append(stem_word)\n",
    "\n",
    "    return tweets_stem\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw tweet is: \n",
      "#nsn_supplements, Effective press release distribution with results! :) [link removed] #PressRelease #NewsDistribution\n",
      "\n",
      "The tweet after removing hashtags and hyperlink is:\n",
      "nsn_supplements, Effective press release distribution with results! :) [link removed] PressRelease NewsDistribution\n",
      " \n",
      "The tweet after tokenizing can be displayed as: \n",
      "['nsn_supplements', ',', 'effective', 'press', 'release', 'distribution', 'with', 'results', '!', ':)', '[', 'link', 'removed', ']', 'pressrelease', 'newsdistribution']\n",
      "\n",
      "The tokens after removing stopwords and punctuation is:\n",
      " ['nsn_supplements', 'effective', 'press', 'release', 'distribution', 'results', ':)', 'link', 'removed', 'pressrelease', 'newsdistribution'] \n",
      "\n",
      "The tokens after stemming them is:\n",
      "['nsn_supplement', 'effect', 'press', 'releas', 'distribut', 'result', ':)', 'link', 'remov', 'pressreleas', 'newsdistribut']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking whether all the processes have worked\n",
    "\n",
    "example_tweet = np.random.randint(0,500)\n",
    "print(\"The raw tweet is: \\n{}\\n\".format(positive_tweets[example_tweet]))\n",
    " \n",
    "print(\"The tweet after removing hashtags and hyperlink is:\\n{}\\n \".format(remove_unwanted(positive_tweets[example_tweet])))\n",
    "\n",
    "print(\"The tweet after tokenizing can be displayed as: \\n{}\\n\".format(tokenize_tweet(remove_unwanted(positive_tweets[example_tweet]))))\n",
    "\n",
    "print(\"The tokens after removing stopwords and punctuation is:\\n {} \\n\".format(remove_stopwords_and_punctuations(tokenize_tweet(remove_unwanted(positive_tweets[example_tweet])))))\n",
    "\n",
    "print(\"The tokens after stemming them is:\\n{}\\n\".format(get_stem(remove_stopwords_and_punctuations(tokenize_tweet(remove_unwanted(positive_tweets[example_tweet]))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since all of these seem to work, we will now apply a combined function of all of there preprocessing techniques to the tweets\n",
    "\n",
    "def combined_preprocessing(tweet):\n",
    "    hashtags_and_links_removed = remove_unwanted(tweet)\n",
    "    tokenized = tokenize_tweet(hashtags_and_links_removed)\n",
    "    stopwords_and_punc_removed = remove_stopwords_and_punctuations(tokenized)\n",
    "    stemmed_tokens = get_stem(stopwords_and_punc_removed)\n",
    "\n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey my fav nirvana song is smells like teen spirit rip amy winehouse :-)))))\n",
      "['hey', 'fav', 'nirvana', 'song', 'smell', 'like', 'teen', 'spirit', 'rip', 'ami', 'winehous', ':-)']\n"
     ]
    }
   ],
   "source": [
    "ex_tweet = np.random.randint(1,5000)\n",
    "print(positive_tweets[ex_tweet])\n",
    "print(combined_preprocessing(positive_tweets[ex_tweet]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting Data into Training and Testing Sets\n",
    "\n",
    "train_positive = positive_tweets[:4000]\n",
    "train_negative = negative_tweets[:4000]\n",
    "test_positive = positive_tweets[4000:]\n",
    "test_negative = negative_tweets[:4000]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
